{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<td>\n",
    "<a href=\"https://colab.research.google.com/github/raoulg/MADS-MachineLearning-course/blob/master/notebooks/4_tuning_networks/01_hypertuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from plotly import graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('dark_background')\n",
    "# import plotly.io as pio\n",
    "# pio.renderers.default = 'plotly_mimetype+notebook'\n",
    "\n",
    "import visualize\n",
    "DELETE = True # to delete the tunedir at the end of the notebook\n",
    "start = time.time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a general reference notebook to explore the use of ray tuner.\n",
    "First, we define some global variables. This makes it easier to change the parameters for the full notebook at once and run everything.\n",
    "\n",
    "Set the max_epochs to something like 10 (this will take longer, but you will get better results), and experiments to 18. This way, you will have a good way to compare gridsearch with the other tuners because gridsearch will do 3 * 6 = 18 experiments. \n",
    "\n",
    "The reason I am setting this low is because i automated the testing of the notebook and it takes a long time to run all the experiments by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A normal variable is a named storage location whose value is expected to change during the program's execution. \n",
    "# A constant is a type of variable whose value is not meant to be changed after it's been assigned.\n",
    "MAX_EPOCHS = 10\n",
    "N_EXPERIMENTS = 18"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train function and config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some dicts to log the results of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = {}\n",
    "best_config = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a training function. This is also implemented in the mltrainer, but I put it here to show you how the details work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, trainstreamer, lossfn, optimizer, steps):\n",
    "    model.train()\n",
    "    train_loss: float = 0.0\n",
    "    for _ in range(steps):\n",
    "        x, y = next(trainstreamer)\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(x)\n",
    "        loss = lossfn(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We validate on the validation set and return the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, validstreamer, lossfn, metric, steps):\n",
    "    model.eval()\n",
    "    valid_loss: float = 0.0\n",
    "    acc: float = 0.0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            x, y = next(validstreamer)\n",
    "            yhat = model(x)\n",
    "            loss = lossfn(yhat, y)\n",
    "            valid_loss += loss.item()\n",
    "            acc += metric(y, yhat)\n",
    "    acc /= steps\n",
    "    return valid_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data requires a bit extra care. Because we will run experiments in parallel on all the cpu's available, we well use FileLock to make sure that loading the data does not conflict.\n",
    "\n",
    "Note we import functions inside the function we will later on pass to ray. This is because ray will serialize the function and send it to the workers. If we import the functions outside the function, the workers will not have access to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(tune_dir):\n",
    "    from filelock import FileLock\n",
    "    from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "    from mltrainer.preprocessors import PaddedPreprocessor\n",
    "    from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "    with FileLock(tune_dir / \".lock\"):\n",
    "        # we lock the datadir to avoid parallel instances trying to\n",
    "        # access the datadir\n",
    "        preprocessor = PaddedPreprocessor()\n",
    "        gesturesdatasetfactory = DatasetFactoryProvider.create_factory(DatasetType.GESTURES)\n",
    "        streamers = gesturesdatasetfactory.create_datastreamer(\n",
    "            batchsize=32, preprocessor=preprocessor\n",
    "        )\n",
    "        train = streamers[\"train\"]\n",
    "        valid = streamers[\"valid\"]\n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same GRU model we used last lesson. You might improve this in a few ways (eg consider adding skip layers, conv1d, etc) but for clarity, lets keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUmodel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: dict,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=config[\"input_size\"],\n",
    "            hidden_size=int(config[\"hidden_size\"]),\n",
    "            dropout=config[\"dropout\"],\n",
    "            batch_first=True,\n",
    "            num_layers=int(config[\"num_layers\"]),\n",
    "        )\n",
    "        self.linear = nn.Linear(int(config[\"hidden_size\"]), config[\"output_size\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        last_step = x[:, -1, :]\n",
    "        yhat = self.linear(last_step)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have all the ingredients we need to run the tuner.\n",
    "We create a function that:\n",
    "- loads the data with a lock\n",
    "- creates the model\n",
    "- trains the model\n",
    "- validates the model\n",
    "- reports the results to ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(config: dict):\n",
    "    from mltrainer.metrics import Accuracy\n",
    "\n",
    "    # load data\n",
    "    train, valid = get_data(config[\"tune_dir\"])\n",
    "    trainsteps = len(train)\n",
    "    validsteps = len(valid)\n",
    "    trainstreamer = train.stream()\n",
    "    validstreamer = valid.stream()\n",
    "\n",
    "    # create model with config\n",
    "    model = GRUmodel(config)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    metric = Accuracy()\n",
    "\n",
    "    for _ in range(config[\"epochs\"]):\n",
    "        # train and validate\n",
    "        train_loss = train_fn(model, trainstreamer, loss_fn, optimizer, trainsteps)\n",
    "        valid_loss, accuracy = validate(model, validstreamer, loss_fn, metric, validsteps)\n",
    "\n",
    "        # report to ray\n",
    "        ray.train.report({\n",
    "            \"valid_loss\": valid_loss / validsteps,\n",
    "            \"train_loss\": train_loss / trainsteps,\n",
    "            \"accuracy\" : accuracy,\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this, to see if everything works as expected.\n",
    "Note that it typically does not make sense to use accelaration here; we will typically have 1 GPU, but 10 or 16 CPUS. We gain much more from parallelizing the experiments than we gain by running the model faster on a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dir = Path(\"logs/test/\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-11 14:57:11.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mFolder already exists at C:\\Users\\r.weenink\\.cache\\mads_datasets\\gestures\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 2600/2600 [00:40<00:00, 63.67it/s]\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 651/651 [00:10<00:00, 64.64it/s]\n",
      "c:\\Users\\r.weenink\\MADS-MachineLearning-course-clone\\MADS-MachineLearning-course\\.venv\\Lib\\site-packages\\ray\\train\\_internal\\session.py:677: UserWarning: `report` is meant to only be called inside a function that is executed by a Tuner or Trainer. Returning `None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"input_size\": 3,\n",
    "    \"output_size\": 20,\n",
    "    \"dropout\": 0.1,\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"epochs\": MAX_EPOCHS,\n",
    "    \"tune_dir\": tune_dir,\n",
    "}\n",
    "tune_model(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do a random search. \n",
    "\n",
    "First, we define the search space. We can specify specific values, but also distributions.\n",
    "For the hidden size, we will use randint, which will sample from a uniform distribution of integers.\n",
    "The same for the number of layers.\n",
    "\n",
    "The `tune.run` function runs the hypertuning. It will sample from the search space, and create a specific config for each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    \"input_size\": 3,\n",
    "    \"output_size\": 20,\n",
    "    \"dropout\": 0.05,\n",
    "    \"epochs\": MAX_EPOCHS,\n",
    "    \"hidden_size\": tune.randint(16, 512),\n",
    "    \"num_layers\": tune.randint(1, 8),\n",
    "    \"tune_dir\": tune_dir,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-11 14:59:54,417\tINFO node.py:355 -- Failed to get node info b'GCS cannot find the node with node ID 175cee9af3e829b9229693e14e93028f3aa8fc3234c0585219a411db. The node registration may not be complete yet before the timeout. Try increase the RAY_raylet_start_wait_time_s config.'\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The current node timed out during startup. This could happen because some of the raylet failed to startup or the GCS has become overloaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m tic = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m analysis = \u001b[43mtune\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtune_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalid_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtune_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_EXPERIMENTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_iteration\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_EPOCHS\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m timer[\u001b[33m\"\u001b[39m\u001b[33mray_random\u001b[39m\u001b[33m\"\u001b[39m] = time.time() - tic\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\r.weenink\\MADS-MachineLearning-course-clone\\MADS-MachineLearning-course\\.venv\\Lib\\site-packages\\ray\\tune\\tune.py:527\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[39m\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    521\u001b[39m     error_message_map = {\n\u001b[32m    522\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mentrypoint\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtune.run(...)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    523\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msearch_space_arg\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    524\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrestore_entrypoint\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtune.run(..., resume=True)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    525\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m \u001b[43m_ray_auto_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_message_map\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mentrypoint\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _remote \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    530\u001b[39m     _remote = ray.util.client.ray.is_connected()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\r.weenink\\MADS-MachineLearning-course-clone\\MADS-MachineLearning-course\\.venv\\Lib\\site-packages\\ray\\tune\\tune.py:252\u001b[39m, in \u001b[36m_ray_auto_init\u001b[39m\u001b[34m(entrypoint)\u001b[39m\n\u001b[32m    250\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTUNE_DISABLE_AUTO_INIT=1\u001b[39m\u001b[33m'\u001b[39m\u001b[33m detected.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ray.is_initialized():\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     \u001b[43mray\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m     logger.info(\n\u001b[32m    254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mInitializing Ray automatically. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFor cluster usage or custom Ray initialization, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    256\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcall `ray.init(...)` before `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentrypoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\r.weenink\\MADS-MachineLearning-course-clone\\MADS-MachineLearning-course\\.venv\\Lib\\site-packages\\ray\\_private\\client_mode_hook.py:104\u001b[39m, in \u001b[36mclient_mode_hook.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func.\u001b[34m__name__\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33minit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\r.weenink\\MADS-MachineLearning-course-clone\\MADS-MachineLearning-course\\.venv\\Lib\\site-packages\\ray\\_private\\worker.py:1911\u001b[39m, in \u001b[36minit\u001b[39m\u001b[34m(address, num_cpus, num_gpus, resources, labels, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, logging_config, log_to_driver, namespace, runtime_env, enable_resource_isolation, system_reserved_cpu, system_reserved_memory, **kwargs)\u001b[39m\n\u001b[32m   1878\u001b[39m     ray_params = ray._private.parameter.RayParams(\n\u001b[32m   1879\u001b[39m         node_ip_address=_node_ip_address,\n\u001b[32m   1880\u001b[39m         driver_mode=driver_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1905\u001b[39m         resource_isolation_config=resource_isolation_config,\n\u001b[32m   1906\u001b[39m     )\n\u001b[32m   1907\u001b[39m     \u001b[38;5;66;03m# Start the Ray processes. We set shutdown_at_exit=False because we\u001b[39;00m\n\u001b[32m   1908\u001b[39m     \u001b[38;5;66;03m# shutdown the node in the ray.shutdown call that happens in the atexit\u001b[39;00m\n\u001b[32m   1909\u001b[39m     \u001b[38;5;66;03m# handler. We still spawn a reaper process in case the atexit handler\u001b[39;00m\n\u001b[32m   1910\u001b[39m     \u001b[38;5;66;03m# isn't called.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m     _global_node = \u001b[43mray\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_private\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m        \u001b[49m\u001b[43mray_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mray_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshutdown_at_exit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspawn_reaper\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mray_init_cluster\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1917\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1918\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1919\u001b[39m     \u001b[38;5;66;03m# In this case, we are connecting to an existing cluster.\u001b[39;00m\n\u001b[32m   1920\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m num_cpus \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m num_gpus \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\r.weenink\\MADS-MachineLearning-course-clone\\MADS-MachineLearning-course\\.venv\\Lib\\site-packages\\ray\\_private\\node.py:357\u001b[39m, in \u001b[36mNode.__init__\u001b[39m\u001b[34m(self, ray_params, head, shutdown_at_exit, spawn_reaper, connect_only, default_worker, ray_init_cluster)\u001b[39m\n\u001b[32m    355\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to get node info \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    356\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m time.monotonic() - start_time > raylet_start_wait_time_s:\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[32m    358\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe current node timed out during startup. This \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    359\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcould happen because some of the raylet failed to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstartup or the GCS has become overloaded.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    361\u001b[39m         )\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# Use node info to update port\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._ray_params.node_manager_port == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mException\u001b[39m: The current node timed out during startup. This could happen because some of the raylet failed to startup or the GCS has become overloaded."
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "analysis = tune.run(\n",
    "    tune_model,\n",
    "    config=config,\n",
    "    metric=\"valid_loss\",\n",
    "    mode=\"min\",\n",
    "    storage_path=str(tune_dir),\n",
    "    num_samples=N_EXPERIMENTS,\n",
    "    stop={\"training_iteration\": MAX_EPOCHS},\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "timer[\"ray_random\"] = time.time() - tic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contour(df, x, y, z, start=0.90, end=1.0, size=0.01):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Contour(\n",
    "            z=df[z],\n",
    "            x=df[x],\n",
    "            y=df[y],\n",
    "            contours=dict(\n",
    "                coloring='heatmap',\n",
    "                showlabels=True,  # show labels on contours\n",
    "                start=start,       # start of the contour range\n",
    "                end=end,          # end of the contour range\n",
    "                size=size,\n",
    "            ),\n",
    "            colorscale=\"plasma\",\n",
    "            colorbar=dict(\n",
    "                title='Accuracy'\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[x],\n",
    "            y=df[y],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color='black',\n",
    "                size=8,\n",
    "                symbol='circle'\n",
    "            ),\n",
    "            customdata=df['accuracy'],  # Pass accuracy values for hover text\n",
    "            hovertemplate=(\n",
    "                'Hidden Size: %{x}<br>'\n",
    "                'Number of Layers: %{y}<br>'\n",
    "                'Accuracy: %{customdata:.4f}<extra></extra>'\n",
    "            ),\n",
    "            name='Data Points'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Contour Plot\",\n",
    "        xaxis_title=\"Hidden Size\",\n",
    "        yaxis_title=\"Number of Layers\",\n",
    "        xaxis=dict(showgrid=False),  # Remove x-axis grid lines\n",
    "        yaxis=dict(showgrid=False),\n",
    "        plot_bgcolor='white',        # Set background color to white\n",
    "        paper_bgcolor='white'\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = analysis.results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = analysis.results_df\n",
    "plot_contour(random, \"config/hidden_size\", \"config/num_layers\", \"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the search space is sort of randomly sampled.\n",
    "Even though big parts are unexplored, it still looks like we find some hotspots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = analysis.get_best_config()\n",
    "best[\"accuracy\"] = analysis.best_result[\"accuracy\"]\n",
    "best_config[\"random\"] = best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we searched the hyperparameter space. Problem is, these spaces potentially can get\n",
    "pretty big. Let's imagine you have 10 hyperparameters, and every hyperparameter has 5\n",
    "possible (relevant) values, you already have $5^{10}$ possible combinations, which is almost 10 million. Even if checking of every configuration would take just 1 second, it would take more than a 100 days to check them all...This\n",
    "space can grow out of control pretty fast.\n",
    "\n",
    "Lets look at the best results we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"config/hidden_size\", \"config/num_layers\", \"accuracy\"]\n",
    "visualize.parallel_plot(analysis, columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the mean scores are sort of randomly distributed. This is a direct\n",
    "effect of random guessing parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this more rigorous with a gridsearch.\n",
    "The upside of this technique is that you will test every configuration.\n",
    "\n",
    "A huge downside is the inefficiency. Also, you will run experiments with combinations that might be not very promising, but very slow, which is pretty inefficient.\n",
    "\n",
    "\n",
    "One way to handle this is to use doubling as a strategy to scan the range: 16, 32, ..., 512. This is a bit more efficient.\n",
    "\n",
    "Typically, this can be a good idea for a first scan, to get a rough idea of the space.\n",
    "After you have done this, you can narrow your searchspace, and do a more fine grained search zoomed in on areas that seem promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"input_size\": 3,\n",
    "    \"output_size\": 20,\n",
    "    \"dropout\": 0.1,\n",
    "    \"epochs\": MAX_EPOCHS,\n",
    "    \"hidden_size\": tune.grid_search([16, 32, 64, 128, 256, 512]),\n",
    "    \"num_layers\": tune.grid_search([2, 4, 8]),\n",
    "    \"tune_dir\": tune_dir,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tic = time.time()\n",
    "\n",
    "analysis = tune.run(\n",
    "    tune_model,\n",
    "    config=config,\n",
    "    metric=\"valid_loss\",\n",
    "    mode=\"min\",\n",
    "    storage_path=str(tune_dir),\n",
    "    stop={\"training_iteration\": MAX_EPOCHS},\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "timer[\"ray_grid\"] = time.time() - tic\n",
    "\n",
    "best = analysis.get_best_config()\n",
    "best[\"accuracy\"] = analysis.best_result[\"accuracy\"]\n",
    "best_config[\"grid\"] = best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_results = pd.concat([all_results, analysis.results_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = analysis.results_df\n",
    "plot_contour(grid, \"config/hidden_size\", \"config/num_layers\", \"accuracy\", start=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we get a systematic scan, but large parts of the space are still unexplored, and we also explored parts that are really not very promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.parallel_plot(analysis, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.plot_timers(timer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we improve the search algorithm with a bayesian optimization.\n",
    "\n",
    "Note that the bayesian search algorithm will only work with continuous parameters. This is a problem for the number of layers, which is a discrete parameter. I fixed this by simply casting the parameters to an integer inside the model, which is not very elegant but it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "\n",
    "bayesopt = BayesOptSearch(metric=\"valid_loss\", mode=\"min\")\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"input_size\": 3,\n",
    "    \"output_size\": 20,\n",
    "    \"dropout\": 0.1,\n",
    "    \"epochs\": MAX_EPOCHS,\n",
    "    \"hidden_size\": tune.uniform(16, 512),\n",
    "    \"num_layers\": tune.uniform(1, 8),\n",
    "    \"tune_dir\": tune_dir,\n",
    "}\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "analysis = tune.run(\n",
    "    tune_model,\n",
    "    config=config,\n",
    "    metric=\"valid_loss\",\n",
    "    mode=\"min\",\n",
    "    storage_path=str(tune_dir),\n",
    "    num_samples=N_EXPERIMENTS,\n",
    "    stop={\"training_iteration\": MAX_EPOCHS},\n",
    "    search_alg=bayesopt,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "timer[\"ray_bayes\"] = time.time() - tic\n",
    "\n",
    "best = analysis.get_best_config()\n",
    "best[\"accuracy\"] = analysis.best_result[\"accuracy\"]\n",
    "best_config[\"bayes\"] = best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([all_results, analysis.results_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes = analysis.results_df\n",
    "plot_contour(bayes, \"config/hidden_size\", \"config/num_layers\", \"accuracy\", start=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, bayes really focuses on the promising areas. It is much more efficient than random search, and also more efficient than grid search.\n",
    "We have set the `random_search_steps` to 5, this means that we will do 5 random searches first, to get a good initial idea of the space. As you can see, 5 is a bit low, because it might lead to premature converging to a local optimum. You can increase this number to get a better initial idea of the space, but you will also need to increase the number of iterations after the initial random scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.parallel_plot(analysis, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.plot_timers(timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame.from_dict(best_config, orient=\"index\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperband\n",
    "\n",
    "Hyperband aborts runs early. Configs that are unpromising are abandoned before they complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "\n",
    "scheduler = AsyncHyperBandScheduler(\n",
    "    time_attr=\"training_iteration\", grace_period=1, reduction_factor=3, max_t=MAX_EPOCHS\n",
    ")\n",
    "\n",
    "config = {\n",
    "    \"input_size\": 3,\n",
    "    \"output_size\": 20,\n",
    "    \"dropout\": 0.1,\n",
    "    \"epochs\": MAX_EPOCHS,\n",
    "    \"hidden_size\": tune.randint(16, 512),\n",
    "    \"num_layers\": tune.randint(1, 8),\n",
    "    \"tune_dir\": tune_dir,\n",
    "}\n",
    "\n",
    "tic = time.time()\n",
    "analysis = tune.run(\n",
    "    tune_model,\n",
    "    config=config,\n",
    "    metric=\"valid_loss\",\n",
    "    mode=\"min\",\n",
    "    storage_path=str(tune_dir),\n",
    "    num_samples=N_EXPERIMENTS,\n",
    "    stop={\"training_iteration\": MAX_EPOCHS},\n",
    "    scheduler=scheduler,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "timer[\"ray_hyperband\"] = time.time() - tic\n",
    "\n",
    "best = analysis.get_best_config()\n",
    "best[\"accuracy\"] = analysis.best_result[\"accuracy\"]\n",
    "best_config[\"hyperband\"] = best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you study the iter column, you will see that some runs have been stopped early. This is exactly the point of hyperband: it tries to allocate resources to the most promising configurations.\n",
    "\n",
    "The downside of this, is that if you try to get a good view of the space, you get distorted results because comparing a model that has trained just 1 or 3 epochs with a model that has trained 100 epochs is not very fair..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([all_results, analysis.results_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperband = analysis.results_df\n",
    "plot_contour(hyperband, \"config/hidden_size\", \"config/num_layers\", \"accuracy\", start=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.parallel_plot(analysis, columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But note that it is faster! This means you could do more runs in the same time, which might be a good tradeoff!\n",
    "Typically you will get better results if you scan the searchspace better by doing more runs, aborting the ones that are not promising and going on with the ones that seem to yield good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.plot_timers(timer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(best_config, orient=\"index\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperbayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to combine Hyperband with Bayesian optimization, implemented as `TuneBOHB` in ray.\n",
    "However, `TuneBOHB` in ray is dependent on `hpbandster`, which is not maintained anymore.\n",
    "Unfortunately, a dependency of `hpbandster` is `netifaces`, which is also not maintained anymore.\n",
    "While `netifaces` does work on some hardware, it fails to build on some other hardware, which is a problem.\n",
    "\n",
    "To still use BOHB type algoritms, I am planning to implement either SMAC3 or BEHB in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperOpt\n",
    "HyperOpt provides gradient/derivative-free optimization. They implemented TPE, which stands for Tree-structured Parzen Estimator. This is a bayesian optimization algorithm that uses a tree to model the distribution of the objective function. The main advantage is that TPE also works with discrete parameters and conditional search spaces which is a problem for the bayesian optimization.\n",
    "\n",
    "For example, you could do things like this:\n",
    "```python\n",
    "conditional_space = {\n",
    "    \"activation\": hp.choice(\n",
    "        \"activation\",\n",
    "        [\n",
    "            {\"activation\": \"relu\", \"mult\": hp.uniform(\"mult\", 1, 2)},\n",
    "            {\"activation\": \"tanh\"},\n",
    "        ],\n",
    "    ),\n",
    "}\n",
    "```\n",
    "\n",
    "Where you have a discrete parameter `activation` that has a conditional parameter `mult`. This means that if you choose `relu`, you also need to set `mult`, but if you choose `tanh`, you do not need to set `mult`. See [documentation of ray](https://docs.ray.io/en/latest/tune/examples/hyperopt_example.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "search = HyperOptSearch()\n",
    "\n",
    "scheduler = AsyncHyperBandScheduler(\n",
    "    time_attr=\"training_iteration\", grace_period=1, reduction_factor=3, max_t=MAX_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"input_size\": 3,\n",
    "    \"output_size\": 20,\n",
    "    \"dropout\": 0.1,\n",
    "    \"epochs\": MAX_EPOCHS,\n",
    "    \"hidden_size\": tune.randint(16, 512),\n",
    "    \"num_layers\": tune.randint(1, 8),\n",
    "    \"tune_dir\": tune_dir,\n",
    "}\n",
    "\n",
    "tic = time.time()\n",
    "analysis = tune.run(\n",
    "    tune_model,\n",
    "    config=config,\n",
    "    metric=\"valid_loss\",\n",
    "    mode=\"min\",\n",
    "    storage_path=str(tune_dir),\n",
    "    num_samples=N_EXPERIMENTS,\n",
    "    stop={\"training_iteration\": MAX_EPOCHS},\n",
    "    search_alg=search,\n",
    "    scheduler=scheduler,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "timer[\"ray_hyperopt\"] = time.time() - tic\n",
    "\n",
    "best = analysis.get_best_config()\n",
    "best[\"accuracy\"] = analysis.best_result[\"accuracy\"]\n",
    "best_config[\"hyperopt\"] = best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([all_results, analysis.results_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperbayes = analysis.results_df\n",
    "plot_contour(hyperbayes, \"config/hidden_size\", \"config/num_layers\", \"accuracy\", start=0.3, size=0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model combines bayesian optimization with hyperband. This is a good idea, because it combines the efficiency of bayesian optimization with the speed of hyperband."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.parallel_plot(analysis, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.plot_timers(timer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(best_config, orient=\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour = all_results[all_results[\"training_iteration\"] == MAX_EPOCHS]\n",
    "plot_contour(contour, \"config/hidden_size\", \"config/num_layers\", \"accuracy\", start=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = time.time() - start\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DELETE:\n",
    "    import shutil\n",
    "    shutil.rmtree(tune_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mads-deeplearning (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
