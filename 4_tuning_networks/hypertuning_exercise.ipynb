{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "651cdafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from pathlib import Path\n",
    "import optuna\n",
    "from optuna.trial import Trial\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d859f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 244\n",
      "Validation samples: 153\n",
      "Number of classes: 2\n",
      "Classes: ['ants', 'bees']\n",
      "Batch shape: torch.Size([4, 3, 224, 224])\n",
      "Labels shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# Define your data directory\n",
    "data_dir = Path.home() / \".cache/mads_datasets/hymenoptera_data/hymenoptera_data\"\n",
    "\n",
    "# Define transforms for training and validation\n",
    "train_transforms = transforms.Compose([\n",
    "    #crop to square first\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),  \n",
    "    transforms.ToTensor(),                    # Convert to tensor\n",
    "    transforms.Normalize(                     # Normalize with ImageNet stats\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "#crop to square first\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),      \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=data_dir / 'train',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    root=data_dir / 'val',\n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True  # Faster data transfer to GPU\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
    "print(f\"Classes: {train_dataset.classes}\")\n",
    "\n",
    "# Test the dataloader\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Batch shape: {images.shape}\")  # Should be [batch_size, 3, 224, 224]\n",
    "print(f\"Labels shape: {labels.shape}\")  # Should be [batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64ca50c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in batch: 4\n"
     ]
    }
   ],
   "source": [
    "#how many images are there in the batch\n",
    "print(f\"Number of images in batch: {images.shape[0]}\")  # Should be batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "260d0293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DynamicSimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes, conv_out_channels, fc_hidden1, fc_hidden2, fc_hidden3, dropout_rate):\n",
    "        \"\"\"\n",
    "        Initializes the dynamic CNN.\n",
    "        \n",
    "        Args:\n",
    "            num_classes (int): Number of output classes.\n",
    "            conv_out_channels (list or tuple): A list of output channel counts \n",
    "                                                for each convolutional block.\n",
    "                                                The length of this list determines\n",
    "                                                the number of conv blocks.\n",
    "            fc_hidden1 (int): Neurons in the 1st hidden FC layer.\n",
    "            fc_hidden2 (int): Neurons in the 2nd hidden FC layer.\n",
    "            fc_hidden3 (int): Neurons in the 3rd hidden FC layer.\n",
    "            dropout_rate (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(DynamicSimpleCNN, self).__init__()\n",
    "        \n",
    "        self.conv_blocks = nn.ModuleList()\n",
    "        self.skip_convs = nn.ModuleList()\n",
    "        self.pools = nn.ModuleList()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        in_c = 3 # Initial input channels (RGB)\n",
    "        \n",
    "        # --- Dynamically create conv blocks ---\n",
    "        for out_c in conv_out_channels:\n",
    "            # Main conv path\n",
    "            self.conv_blocks.append(\n",
    "                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "            )\n",
    "            \n",
    "            # Skip connection path (1x1 conv for channel matching)\n",
    "            self.skip_convs.append(\n",
    "                nn.Conv2d(in_c, out_c, kernel_size=1, stride=1)\n",
    "            )\n",
    "            \n",
    "            # Standardized pooling layer after each block\n",
    "            self.pools.append(\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "            \n",
    "            # Update in_c for the next block\n",
    "            in_c = out_c\n",
    "            \n",
    "        \n",
    "        # Classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # The input to the classifier is the last output channel count\n",
    "        final_conv_out = conv_out_channels[-1]\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(final_conv_out, fc_hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_hidden1, fc_hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_hidden2, fc_hidden3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(fc_hidden3, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # --- Pass through dynamic conv blocks ---\n",
    "        for i in range(len(self.conv_blocks)):\n",
    "            identity = x\n",
    "            \n",
    "            # Main path\n",
    "            out = self.relu(self.conv_blocks[i](x))\n",
    "            \n",
    "            # Skip path\n",
    "            skip = self.skip_convs[i](identity)\n",
    "            \n",
    "            # Add and activate\n",
    "            x = self.relu(out + skip)\n",
    "            \n",
    "            # Pool\n",
    "            x = self.pools[i](x)\n",
    "            \n",
    "        \n",
    "        # --- Classifier ---\n",
    "        x = self.avgpool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9101b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Your original class (for comparison)\n",
    "# class DynamicSimpleCNN(nn.Module): ...\n",
    "\n",
    "class DynamicClassifierWithTransferLearning(nn.Module):\n",
    "    def __init__(self, num_classes, fc_hidden1, fc_hidden2, fc_hidden3, \n",
    "                 dropout_rate, freeze_base_model=True):\n",
    "        \"\"\"\n",
    "        Initializes the model with a pre-trained base.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Number of output classes.\n",
    "            fc_hidden1 (int): Neurons in the 1st hidden FC layer.\n",
    "            fc_hidden2 (int): Neurons in the 2nd hidden FC layer.\n",
    "            fc_hidden3 (int): Neurons in the 3rd hidden FC layer.\n",
    "            dropout_rate (float): Dropout probability.\n",
    "            freeze_base_model (bool): If True, freeze the weights of the\n",
    "                                      pre-trained base model.\n",
    "        \"\"\"\n",
    "        super(DynamicClassifierWithTransferLearning, self).__init__()\n",
    "        \n",
    "        # 1. Load a pre-trained base model (e.g., ResNet-50)\n",
    "        # We use the recommended modern weights API\n",
    "        self.base_model = models.resnet50(\n",
    "            weights=models.ResNet50_Weights.DEFAULT\n",
    "        )\n",
    "\n",
    "        # 2. Freeze the base model's parameters\n",
    "        if freeze_base_model:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # 3. Get the number of input features for the original classifier\n",
    "        # For ResNet-50, this is 2048\n",
    "        num_ftrs = self.base_model.fc.in_features\n",
    "        \n",
    "        # 4. Create your dynamic classifier\n",
    "        # We replace the original model's nn.Flatten() because\n",
    "        # the ResNet forward pass already flattens the features\n",
    "        # right before the 'fc' layer.\n",
    "        self.custom_classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, fc_hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_hidden1, fc_hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_hidden2, fc_hidden3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(fc_hidden3, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 5. Replace the base model's final layer with our new classifier\n",
    "        self.base_model.fc = self.custom_classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        The base_model already includes the AdaptiveAvgPool and Flatten.\n",
    "        \"\"\"\n",
    "        return self.base_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15e17a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= TRAINING FUNCTIONS =============\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4acdb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-26 19:42:46,741] A new study created in memory with name: no-name-b8e25446-f9b5-483e-94db-4aa1e4bf96ba\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to C:\\Users\\r.weenink/.cache\\torch\\hub\\checkpoints\\resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97.8M/97.8M [00:01<00:00, 88.9MB/s]\n",
      "[I 2025-10-26 19:43:35,310] Trial 0 finished with value: 0.8758169934640523 and parameters: {'lr': 0.0014453363307990706, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.2396759262456736, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.8758169934640523.\n",
      "[I 2025-10-26 19:44:29,778] Trial 1 finished with value: 0.9607843137254902 and parameters: {'lr': 0.0073922186469834285, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.15497484895637012, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.9607843137254902.\n",
      "[I 2025-10-26 19:45:24,366] Trial 2 finished with value: 0.9673202614379085 and parameters: {'lr': 0.0016090829719801736, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.19819771880335438, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 19:46:19,953] Trial 3 finished with value: 0.9477124183006536 and parameters: {'lr': 0.0022671684011315754, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.23378438339146781, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 19:47:15,893] Trial 4 finished with value: 0.869281045751634 and parameters: {'lr': 0.006108086638691169, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.2400329238559322, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 19:48:14,275] Trial 5 finished with value: 0.9477124183006536 and parameters: {'lr': 0.004527472708109655, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.24589698270621085, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 19:49:01,909] Trial 6 pruned. \n",
      "[I 2025-10-26 19:49:47,325] Trial 7 pruned. \n",
      "[I 2025-10-26 19:50:42,392] Trial 8 finished with value: 0.9411764705882353 and parameters: {'lr': 0.006555971569863297, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.23963332070249727, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 19:51:36,417] Trial 9 finished with value: 0.9673202614379085 and parameters: {'lr': 0.0016623781249630258, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.24064927570691533, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 19:52:30,999] Trial 10 finished with value: 0.9411764705882353 and parameters: {'lr': 0.0010097179983368835, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.19003338648704163, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 19:53:26,190] Trial 11 finished with value: 0.9607843137254902 and parameters: {'lr': 0.002367044166381383, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.20471403554650874, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 19:54:20,806] Trial 12 finished with value: 0.954248366013072 and parameters: {'lr': 0.0016688704557803356, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.20503325960646288, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 19:55:17,157] Trial 13 finished with value: 0.8627450980392157 and parameters: {'lr': 0.003545329773213594, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.1836526833820032, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 19:56:12,555] Trial 14 finished with value: 0.9673202614379085 and parameters: {'lr': 0.0010326162402161583, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.2172609006114819, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 19:57:07,770] Trial 15 finished with value: 0.9411764705882353 and parameters: {'lr': 0.0018841503571337638, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.18431930186363196, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 19:57:52,280] Trial 16 pruned. \n",
      "[I 2025-10-26 19:58:47,085] Trial 17 finished with value: 0.954248366013072 and parameters: {'lr': 0.0028213611621582626, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.21797040357140146, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 19:59:42,665] Trial 18 finished with value: 0.954248366013072 and parameters: {'lr': 0.001320466026844072, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.21894467141275065, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:00:26,303] Trial 19 pruned. \n",
      "[I 2025-10-26 20:01:21,243] Trial 20 finished with value: 0.9477124183006536 and parameters: {'lr': 0.0012993048982283565, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.19725737691828077, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:02:06,027] Trial 21 pruned. \n",
      "[I 2025-10-26 20:03:01,724] Trial 22 finished with value: 0.9411764705882353 and parameters: {'lr': 0.0017796246305418746, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.21157873843957414, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:03:56,966] Trial 23 finished with value: 0.9673202614379085 and parameters: {'lr': 0.0012293800164814806, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.22857479397500338, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:04:52,027] Trial 24 finished with value: 0.9607843137254902 and parameters: {'lr': 0.0021577599200970628, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.1964240812867788, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:05:46,951] Trial 25 finished with value: 0.8627450980392157 and parameters: {'lr': 0.0015524375353161049, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.24964995177482985, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:06:43,132] Trial 26 finished with value: 0.9477124183006536 and parameters: {'lr': 0.0010840529020726256, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.20984589423159308, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:07:41,224] Trial 27 finished with value: 0.934640522875817 and parameters: {'lr': 0.0019498865076129114, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.22494681946111678, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:08:39,075] Trial 28 finished with value: 0.9673202614379085 and parameters: {'lr': 0.0028568231130919205, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.19104694555582835, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:09:36,295] Trial 29 finished with value: 0.9411764705882353 and parameters: {'lr': 0.0015610140448845404, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.21441684548537734, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:10:34,054] Trial 30 finished with value: 0.9411764705882353 and parameters: {'lr': 0.0011856457687931716, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.17355080827811456, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:11:31,819] Trial 31 finished with value: 0.9673202614379085 and parameters: {'lr': 0.0013752431393987622, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.22652984885998048, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:12:29,137] Trial 32 finished with value: 0.8954248366013072 and parameters: {'lr': 0.0012225921575577464, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.23182278258166086, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:13:26,380] Trial 33 finished with value: 0.954248366013072 and parameters: {'lr': 0.0015160555782862716, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.22285135226987005, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:14:21,433] Trial 34 finished with value: 0.9281045751633987 and parameters: {'lr': 0.0011921108727717125, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.23212630605492462, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:15:06,009] Trial 35 pruned. \n",
      "[I 2025-10-26 20:16:01,581] Trial 36 finished with value: 0.9411764705882353 and parameters: {'lr': 0.002063200984494032, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.23849780261549075, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:16:57,873] Trial 37 finished with value: 0.954248366013072 and parameters: {'lr': 0.0014376737541604244, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.2050037680201634, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:17:55,404] Trial 38 finished with value: 0.9607843137254902 and parameters: {'lr': 0.0011476499112362622, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.23448179440418254, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:18:52,970] Trial 39 finished with value: 0.934640522875817 and parameters: {'lr': 0.004405401476882887, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.24732578284313197, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:19:39,058] Trial 40 pruned. \n",
      "[I 2025-10-26 20:20:25,862] Trial 41 pruned. \n",
      "[I 2025-10-26 20:21:21,649] Trial 42 finished with value: 0.9411764705882353 and parameters: {'lr': 0.002468282006240752, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.18710851572799475, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:22:17,198] Trial 43 finished with value: 0.9607843137254902 and parameters: {'lr': 0.0046600984480603975, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.2008466767841993, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:23:13,025] Trial 44 finished with value: 0.954248366013072 and parameters: {'lr': 0.002833528157774996, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.1786888740303258, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:24:07,777] Trial 45 finished with value: 0.9607843137254902 and parameters: {'lr': 0.0010569286092257134, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.23645695036415257, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:25:04,607] Trial 46 finished with value: 0.9215686274509803 and parameters: {'lr': 0.0017925436783868868, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.20757353762664332, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:26:04,362] Trial 47 finished with value: 0.9281045751633987 and parameters: {'lr': 0.0036679485970258553, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.20092838893006343, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n",
      "[I 2025-10-26 20:27:04,141] Trial 48 pruned. \n",
      "[I 2025-10-26 20:27:59,549] Trial 49 finished with value: 0.9607843137254902 and parameters: {'lr': 0.0012688670271588838, 'batch_size': 4, 'fc_hidden1': 512, 'fc_hidden2': 256, 'fc_hidden3': 128, 'dropout_rate': 0.2444614619231721, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.9673202614379085.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All 50 trials saved to 'optuna_trials_results.csv'\n",
      "\n",
      "==================================================\n",
      "Best trial:\n",
      "  Trial Number: 2\n",
      "  Value (Val Accuracy): 0.9673\n",
      "  Params:\n",
      "    lr: 0.0016090829719801736\n",
      "    batch_size: 4\n",
      "    fc_hidden1: 512\n",
      "    fc_hidden2: 256\n",
      "    fc_hidden3: 128\n",
      "    dropout_rate: 0.19819771880335438\n",
      "    optimizer: Adam\n",
      "\n",
      "Best hyperparameters saved to 'best_hyperparameters.json'\n",
      "\n",
      "Visualizations saved:\n",
      "  - optimization_history.html\n",
      "  - param_importances.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============= OBJECTIVE FUNCTION FOR OPTUNA =============\n",
    "def objective(trial: Trial):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_float('lr', 1e-3, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [4])\n",
    "    # conv1_out = trial.suggest_categorical('conv1_out', [64])\n",
    "    # conv2_out = trial.suggest_categorical('conv2_out', [128])\n",
    "    # conv3_out = trial.suggest_categorical('conv3_out', [192])\n",
    "    # conv4_out = trial.suggest_categorical('conv4_out', [200,256])\n",
    "    # conv5_out = trial.suggest_categorical('conv5_out', [400, 512])\n",
    "    fc_hidden1 = trial.suggest_categorical('fc_hidden1', [512])\n",
    "    fc_hidden2 = trial.suggest_categorical('fc_hidden2', [256])\n",
    "    fc_hidden3 = trial.suggest_categorical('fc_hidden3', [128])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.15, 0.25)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam'])\n",
    "    \n",
    "    # Create dataloaders with suggested batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Create model with suggested architecture\n",
    "    model = DynamicClassifierWithTransferLearning(\n",
    "        num_classes=num_classes,\n",
    "        fc_hidden1=fc_hidden1,\n",
    "        fc_hidden2=fc_hidden2,\n",
    "        fc_hidden3=fc_hidden3,\n",
    "        dropout_rate=dropout_rate\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create optimizer based on suggestion\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "        \n",
    "    \n",
    "    # Training loop (reduced epochs for faster tuning)\n",
    "    num_epochs = 5\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Report intermediate value for pruning\n",
    "        trial.report(val_acc, epoch)\n",
    "        \n",
    "        # Prune trial if it's not promising\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return val_acc\n",
    "\n",
    "# ============= RUN HYPERPARAMETER SEARCH =============\n",
    "if __name__ == '__main__':\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=3)\n",
    "    )\n",
    "    \n",
    "    # Run optimization\n",
    "    print(\"Starting hyperparameter optimization...\")\n",
    "    study.optimize(objective, n_trials=50, timeout=None)\n",
    "    \n",
    "    # Convert trials to DataFrame\n",
    "    trials_data = []\n",
    "    for trial in study.trials:\n",
    "        trial_dict = {\n",
    "            'trial_number': trial.number,\n",
    "            'val_accuracy': trial.value if trial.value is not None else 0.0,\n",
    "            'state': trial.state.name,\n",
    "            'duration_seconds': trial.duration.total_seconds() if trial.duration else 0.0,\n",
    "        }\n",
    "        # Add all hyperparameters\n",
    "        trial_dict.update(trial.params)\n",
    "        trials_data.append(trial_dict)\n",
    "    \n",
    "    df_results = pd.DataFrame(trials_data)\n",
    "    \n",
    "    # Sort by validation accuracy\n",
    "    df_results = df_results.sort_values('val_accuracy', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df_results.to_csv('optuna_trials_results.csv', index=False)\n",
    "    print(f\"\\nAll {len(df_results)} trials saved to 'optuna_trials_results.csv'\")\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"  Trial Number: {trial.number}\")\n",
    "    print(f\"  Value (Val Accuracy): {trial.value:.4f}\")\n",
    "    print(\"  Params:\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    \n",
    "    # Save best hyperparameters\n",
    "    import json\n",
    "    with open('best_hyperparameters.json', 'w') as f:\n",
    "        json.dump(trial.params, f, indent=4)\n",
    "    print(\"\\nBest hyperparameters saved to 'best_hyperparameters.json'\")\n",
    "    \n",
    "    # Optionally visualize\n",
    "    try:\n",
    "        import optuna.visualization as vis\n",
    "        fig = vis.plot_optimization_history(study)\n",
    "        fig.write_html('optimization_history.html')\n",
    "        \n",
    "        fig = vis.plot_param_importances(study)\n",
    "        fig.write_html('param_importances.html')\n",
    "        \n",
    "        print(\"\\nVisualizations saved:\")\n",
    "        print(\"  - optimization_history.html\")\n",
    "        print(\"  - param_importances.html\")\n",
    "    except:\n",
    "        print(\"Install plotly for visualization: pip install plotly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8ffc622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_number</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>state</th>\n",
       "      <th>duration_seconds</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>fc_hidden1</th>\n",
       "      <th>fc_hidden2</th>\n",
       "      <th>fc_hidden3</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.967320</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>54.024678</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.240649</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.967320</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>54.587536</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.198198</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>0.967320</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>55.397840</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.217261</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>0.967320</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>57.764627</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.226530</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>0.967320</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>57.850347</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.191047</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23</td>\n",
       "      <td>0.967320</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>55.234899</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.228575</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>55.060538</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.196424</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>54.466504</td>\n",
       "      <td>0.007392</td>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.154975</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>55.189832</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.204714</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>43</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>55.549556</td>\n",
       "      <td>0.004660</td>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.200847</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trial_number  val_accuracy     state  duration_seconds        lr  \\\n",
       "0             9      0.967320  COMPLETE         54.024678  0.001662   \n",
       "1             2      0.967320  COMPLETE         54.587536  0.001609   \n",
       "2            14      0.967320  COMPLETE         55.397840  0.001033   \n",
       "3            31      0.967320  COMPLETE         57.764627  0.001375   \n",
       "4            28      0.967320  COMPLETE         57.850347  0.002857   \n",
       "5            23      0.967320  COMPLETE         55.234899  0.001229   \n",
       "6            24      0.960784  COMPLETE         55.060538  0.002158   \n",
       "7             1      0.960784  COMPLETE         54.466504  0.007392   \n",
       "8            11      0.960784  COMPLETE         55.189832  0.002367   \n",
       "9            43      0.960784  COMPLETE         55.549556  0.004660   \n",
       "\n",
       "   batch_size  fc_hidden1  fc_hidden2  fc_hidden3  dropout_rate optimizer  \n",
       "0           4         512         256         128      0.240649      Adam  \n",
       "1           4         512         256         128      0.198198      Adam  \n",
       "2           4         512         256         128      0.217261      Adam  \n",
       "3           4         512         256         128      0.226530      Adam  \n",
       "4           4         512         256         128      0.191047      Adam  \n",
       "5           4         512         256         128      0.228575      Adam  \n",
       "6           4         512         256         128      0.196424      Adam  \n",
       "7           4         512         256         128      0.154975      Adam  \n",
       "8           4         512         256         128      0.204714      Adam  \n",
       "9           4         512         256         128      0.200847      Adam  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mads-deeplearning (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
